{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import transformers\n",
    "\n",
    "import transformers\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset\n",
    "from transformers import BertTokenizer,BertModel\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ExponentialLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import DebertaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(f\"../data/fe_data/fe_train.csv\")\n",
    "test_df = pd.read_csv(f\"../data/fe_data/fe_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pd.read_csv(f\"../data/stumbleupon/sampleSubmission.csv\", sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"total_text\"] = train_df[\"total_text\"].str.lower()\n",
    "test_df[\"total_text\"] = test_df[\"total_text\"].str.lower()\n",
    "\n",
    "\n",
    "text_features = [\"total_text\"]\n",
    "xtrain = train_df[text_features + [\"label\"]]\n",
    "\n",
    "xtest = test_df[text_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "print(torch.backends.mps.is_available())\n",
    "\n",
    "print(torch.backends.mps.is_built())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {  \"model_name\" :\"bert-base-uncased\",\n",
    "            \"TOKENIZER\"  :BertTokenizer.from_pretrained(\"bert-base-uncased\", \n",
    "                                                        do_lower_case=True,),\n",
    "            \"MAX_LEN\" : 512, #128, #64\n",
    "            \"TRAIN_BATCH_SIZE\" : 2,  #6\n",
    "            \"EPOCHS\" : 2,\n",
    "            \"DEVICE\" : \"mps\",\n",
    "            \"MODEL_PATH\":\"model.pth\"\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTDataset:\n",
    "    def __init__(self, text, target):\n",
    "        self.text = text\n",
    "        self.target = target\n",
    "        self.tokenizer = config[\"TOKENIZER\"]\n",
    "        self.max_len = config[\"MAX_LEN\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = str(self.text[item])\n",
    "        text = \" \".join(text.split())\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "#             pad_to_max_length=True,\n",
    "            truncation=True,\n",
    "            padding='max_length'\n",
    "        )\n",
    "        ids = inputs[\"input_ids\"]\n",
    "        mask = inputs[\"attention_mask\"]\n",
    "        if(\"token_type_ids\" in inputs.keys()):\n",
    "            token_type_ids = inputs[\"token_type_ids\"]\n",
    "            return {\n",
    "            \"ids\": torch.tensor(ids, dtype=torch.long),\n",
    "            \"mask\": torch.tensor(mask, dtype=torch.long),\n",
    "            \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            \"targets\": torch.tensor(self.target[item], dtype=torch.float),\n",
    "        }\n",
    "        else:\n",
    "            return {\n",
    "                \"ids\": torch.tensor(ids, dtype=torch.long),\n",
    "                \"mask\": torch.tensor(mask, dtype=torch.long),\n",
    "                \"targets\": torch.tensor(self.target[item], dtype=torch.float),\n",
    "            }\n",
    "\n",
    "class BERT_Test_Dataset:\n",
    "    def __init__(self, text):\n",
    "        self.text = text\n",
    "        self.tokenizer = config[\"TOKENIZER\"]\n",
    "        self.max_len = config[\"MAX_LEN\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = str(self.text[item])\n",
    "        text = \" \".join(text.split())\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "#             pad_to_max_length=True,\n",
    "            truncation=True,\n",
    "            padding='max_length'\n",
    "        )\n",
    "        ids = inputs[\"input_ids\"]\n",
    "        mask = inputs[\"attention_mask\"]\n",
    "        if(\"token_type_ids\" in inputs.keys()):\n",
    "            token_type_ids = inputs[\"token_type_ids\"]\n",
    "            return {\n",
    "            \"ids\": torch.tensor(ids, dtype=torch.long),\n",
    "            \"mask\": torch.tensor(mask, dtype=torch.long),\n",
    "            \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n",
    "        }\n",
    "        else:\n",
    "            return {\n",
    "                \"ids\": torch.tensor(ids, dtype=torch.long),\n",
    "                \"mask\": torch.tensor(mask, dtype=torch.long),\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "class TextClassification(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(TextClassification, self).__init__()\n",
    "        self.bert = transformers.BertModel.from_pretrained(pretrained_model_name_or_path =config[\"model_name\"], return_dict=False, )\n",
    "        self.bert_drop = nn.Dropout(0.3)\n",
    "        self.out = nn.Linear(768, 1)\n",
    "\n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        _, o2 = self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids)\n",
    "        bo = self.bert_drop(o2)\n",
    "        output = self.out(bo)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "def loss_fn(outputs, targets):\n",
    "    return nn.BCEWithLogitsLoss()(outputs, targets.view(-1, 1))\n",
    "\n",
    "\n",
    "def train_fn(data_loader, model, optimizer, device, scheduler, epoch,fold):\n",
    "    model.train()\n",
    "    loss_train_total = 0\n",
    "    \n",
    "    progress_bar = tqdm(enumerate(data_loader), \n",
    "                        total=len(data_loader),\n",
    "                        desc='OOF {:1d} Epoch {:1d}'.format(int(fold) , epoch), \n",
    "                        leave=False, \n",
    "                        disable=False)    \n",
    "\n",
    "    for bi, d in  progress_bar:\n",
    "        ids = d[\"ids\"]\n",
    "        mask = d[\"mask\"]\n",
    "        targets = d[\"targets\"]\n",
    "\n",
    "        ids = ids.to(device, dtype=torch.long)\n",
    "        mask = mask.to(device, dtype=torch.long)\n",
    "        targets = targets.to(device, dtype=torch.float)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        if(\"token_type_ids\" in d.keys()):\n",
    "            token_type_ids = d[\"token_type_ids\"].to(device, dtype=torch.long)\n",
    "            outputs = model(ids=ids, mask=mask, token_type_ids=token_type_ids)\n",
    "        else:\n",
    "            outputs = model(ids=ids, mask=mask)\n",
    "\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        loss_train_total +=loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        progress_bar.set_postfix({'training_loss': '{:.5f}'.format(loss.item()/len(targets))})\n",
    "    loss_train_avg = loss_train_total/len(data_loader)\n",
    "    return loss_train_avg \n",
    "\n",
    "\n",
    "def eval_fn(data_loader, model, device):\n",
    "    model.eval()\n",
    "    fin_targets = []\n",
    "    fin_outputs = []\n",
    "    loss_total = 0\n",
    "    with torch.no_grad():\n",
    "        for bi, d in tqdm(enumerate(data_loader), total=len(data_loader)):\n",
    "            ids = d[\"ids\"]\n",
    "        \n",
    "            \n",
    "            mask = d[\"mask\"]\n",
    "            targets = d[\"targets\"]\n",
    "\n",
    "            ids = ids.to(device, dtype=torch.long)\n",
    "            mask = mask.to(device, dtype=torch.long)\n",
    "            targets = targets.to(device, dtype=torch.float)\n",
    "            if(\"token_type_ids\" in d.keys()):\n",
    "                token_type_ids = d[\"token_type_ids\"].to(device, dtype=torch.long)\n",
    "                outputs = model(ids=ids, mask=mask, token_type_ids=token_type_ids)\n",
    "            else:\n",
    "                outputs = model(ids=ids, mask=mask)\n",
    "            \n",
    "                loss_total = loss_total + loss_fn(outputs, targets).item()\n",
    "            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
    "            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
    "            loss_total = loss_total / len(data_loader)\n",
    "    return loss_total,fin_outputs, fin_targets\n",
    "\n",
    "def eval_test(data_loader, model, device):\n",
    "    model.eval()\n",
    "    fin_outputs = []\n",
    "    progress_bar = tqdm(enumerate(data_loader), \n",
    "                        total=len(data_loader),\n",
    "                        desc='Generating Test Output'.format(epoch), \n",
    "                        leave=False, \n",
    "                        disable=False)\n",
    "    with torch.no_grad():\n",
    "        for bi, d in progress_bar:\n",
    "            ids = d[\"ids\"]\n",
    "            mask = d[\"mask\"]\n",
    "\n",
    "            ids = ids.to(device, dtype=torch.long)\n",
    "            mask = mask.to(device, dtype=torch.long)\n",
    "            if(\"token_type_ids\" in d.keys()):\n",
    "                token_type_ids = d[\"token_type_ids\"].to(device, dtype=torch.long)\n",
    "                outputs = model(ids=ids, mask=mask, token_type_ids=token_type_ids)\n",
    "            else:\n",
    "                outputs = model(ids=ids, mask=mask)\n",
    "            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
    "    return fin_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_data_loaders(x_train,x_valid):\n",
    "#     x_train , x_valid = train_test_split(train, test_size=0.1,random_state=2020)\n",
    "    train_dataset = BERTDataset(text=x_train.total_text.values, target=x_train.label.values)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset,batch_size = config[\"TRAIN_BATCH_SIZE\"],shuffle=True)\n",
    "    \n",
    "    valid_dataset = BERTDataset(text=x_valid.total_text.values, target=x_valid.label.values)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid_dataset,batch_size = config[\"TRAIN_BATCH_SIZE\"],shuffle=True)\n",
    "    \n",
    "    return train_loader , valid_loader\n",
    "\n",
    "def get_test_data_loaders(x_test):\n",
    "#     x_train , x_valid = train_test_split(train, test_size=0.1,random_state=2020)\n",
    "    test_dataset = BERT_Test_Dataset(text=x_test.total_text.values)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset,batch_size = config[\"TRAIN_BATCH_SIZE\"],shuffle=False)\n",
    "    \n",
    "    return test_loader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(config[\"DEVICE\"])\n",
    "num_folds = 2\n",
    "kf = model_selection.StratifiedKFold(n_splits=num_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zc/1251ld6d5gv27qfz8382s_rh0000gn/T/ipykernel_11234/4211825696.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xtrain.loc[v_, 'kfold'] = f\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7167544c66594806b7a9f45cc40498c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d64866665784dc69a72902bd771ce1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "OOF 0 Epoch 0:   0%|          | 0/1849 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0\n",
      "Training loss: 0.4735103784609544\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cda972f9a6641629abe089f9a247722",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1849 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0\n",
      "AUC : 0.8710771572415408\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b739ad914c2492787960fd7423f7987",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "OOF 0 Epoch 1:   0%|          | 0/1849 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1\n",
      "Training loss: 0.38381016951431096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8d667d73c414d27b9d42c3e71681dd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1849 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0\n",
      "AUC : 0.872728017796511\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0685b00f80264c6fab30396c767ab891",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Test Output:   0%|          | 0/1586 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF -- 0.0 ROC AUC Score = 0.872728017796511\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5042b22357374e78bab756fd6be87983",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74ecb1e7c0a44ecb8396abe786941566",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "OOF 1 Epoch 0:   0%|          | 0/1849 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0\n",
      "Training loss: 0.49307335186054607\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5041380b2ab742149d1899c9409fa1bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1849 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0\n",
      "AUC : 0.8811976094903444\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9217a2bb6a384bbe8e805830fc79806d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "OOF 1 Epoch 1:   0%|          | 0/1849 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1\n",
      "Training loss: 0.40189161261049006\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5b9195e66384593b82bc4cdd6780640",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1849 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0\n",
      "AUC : 0.8850722594392975\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4767f3549c8a4514b04f08f741cf7702",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Test Output:   0%|          | 0/1586 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF -- 1.0 ROC AUC Score = 0.8850722594392975\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# fill the new kfold column\n",
    "for f, (t_, v_) in enumerate(kf.split(X=xtrain, y=xtrain[\"label\"])):\n",
    "    xtrain.loc[v_, 'kfold'] = f\n",
    "\n",
    "val_auc = 0\n",
    "y_test_pred = []\n",
    "for fold in xtrain.kfold.unique():\n",
    "\n",
    "    model = TextClassification()\n",
    "    model = nn.DataParallel(model)\n",
    "    model.to(device)\n",
    "    x_train = xtrain.loc[xtrain.kfold != fold,:].reset_index(drop=True)\n",
    "    x_valid = xtrain.loc[xtrain.kfold == fold,:].reset_index(drop=True)\n",
    "\n",
    "    train_data_loader , valid_data_loader = get_data_loaders(x_train,x_valid)\n",
    "\n",
    "    num_train_steps = int(len(x_train) / config[\"TRAIN_BATCH_SIZE\"] * config[\"EPOCHS\"])\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=0, num_training_steps=num_train_steps\n",
    "    )\n",
    "\n",
    "\n",
    "    \n",
    "    for epoch in tqdm(range(config[\"EPOCHS\"])):\n",
    "\n",
    "        loss_train_avg = train_fn(train_data_loader, model, optimizer, device, scheduler,epoch,fold)\n",
    "        tqdm.write(f'\\nEpoch {epoch}')\n",
    "        tqdm.write(f'Training loss: {loss_train_avg}')\n",
    "        val_loss,outputs, targets = eval_fn(valid_data_loader, model, device)\n",
    "        auc = metrics.roc_auc_score(targets, outputs)     \n",
    "        tqdm.write(f'Validation Loss: {val_loss}')\n",
    "        tqdm.write(f'AUC : {auc}')        \n",
    "\n",
    "    val_auc = val_auc + auc\n",
    "    test_data_loader = get_test_data_loaders(xtest)\n",
    "    outputs = eval_test(test_data_loader, model, device)\n",
    "    y_test_pred.append(outputs)\n",
    "    tqdm.write(f\"OOF -- {fold} ROC AUC Score = {auc}\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Val AUC -- 0.43945006930895214\n"
     ]
    }
   ],
   "source": [
    "val_auc_avg = val_auc / 4\n",
    "print(f\"Total Val AUC -- {val_auc_avg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): TextClassification(\n",
       "    (bert): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): BertPooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (bert_drop): Dropout(p=0.3, inplace=False)\n",
       "    (out): Linear(in_features=768, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE = 'model_v1.pt'\n",
    "torch.save(model, FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model\n",
    "#FILE = 'model_v1.pt'\n",
    "#model = torch.load(FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df[\"label\"] = 0\n",
    "bert_pred = 0\n",
    "for pred in y_test_pred:\n",
    "    bert_pred += np.array(pred).flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df[\"label\"] = bert_pred\n",
    "submission_df.to_csv(f\"../data/submission/submission_bert.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
